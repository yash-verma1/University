---
title: "Assignment 5 - Classification"
author: "Yash Verma"
date: "November 29, 2016"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Question 1.

##(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?

```{r}
library(ISLR)
summary(Weekly)
pairs(Weekly)
cor(Weekly[,-9])
```

###Year and Volume appear to have a relationship. No other patterns are discernible.

##(b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
attach(Weekly)
glm.fit <-  glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                data = Weekly,
                family = binomial)
summary(glm.fit)
```
###Lag 2 appears to have some statistical significance with a Pr(>|z|) = 3%.

##(c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.

```{r}
glm.probs <-  predict(glm.fit, type="response")
glm.pred <-  rep("Down", length(glm.probs))
glm.pred[glm.probs>.5] <-  "Up"
table(glm.pred, Direction)
```

###Percentage of currect predictions: (54+557)/(54+557+48+430) = 56.1%. Weeks the market goes up the logistic regression is right most of the time, 557/(557+48) = 92.1%. Weeks the market goes up the logistic regression is wrong most of the time 54/(430+54) = 11.2%.

##(d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).

```{r}
train <-  (Year < 2009)
Weekly.0910 <-  Weekly[!train,]
glm.fit <-  glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs <-  predict(glm.fit, Weekly.0910, type="response")
glm.pred <-  rep("Down", length(glm.probs))
glm.pred[glm.probs>.5] <-  "Up"
Direction.0910 <-  Direction[!train]
table(glm.pred, Direction.0910)
mean(glm.pred == Direction.0910)
```

##(e) Repeat (d) using LDA.

```{r}
library(MASS)
lda.fit <-  lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.pred <-  predict(lda.fit, Weekly.0910)
table(lda.pred$class, Direction.0910)
mean(lda.pred$class == Direction.0910)
```

##(f) Repeat (d) using QDA.

```{r}
qda.fit <-  qda(Direction ~ Lag2, data = Weekly, subset = train)
qda.class <-  predict(qda.fit, Weekly.0910)$class
table(qda.class, Direction.0910)
mean(qda.class == Direction.0910)
```
###A correctness of 58.7% even though it picked Up the whole time.

##(g) Repeat (d) using KNN with K = 1.

```{r}
library(class)
train.X <-  as.matrix(Lag2[train])
test.X <-  as.matrix(Lag2[!train])
train.Direction <-  Direction[train]
set.seed(1)
knn.pred <-  knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)
```

##(h) Which of these methods appears to provide the best results on this data?
###Logistic regression and LDA methods provide similar test error rates.

##(i) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.

###Logistic regression with Lag2:Lag1
```{r}
glm.fit <-  glm(Direction~Lag2:Lag1, data = Weekly, family = binomial, subset = train)
glm.probs <-  predict(glm.fit, Weekly.0910, type = "response")
glm.pred <-  rep("Down", length(glm.probs))
glm.pred[glm.probs>.5] <-  "Up"
Direction.0910 <-  Direction[!train]
table(glm.pred, Direction.0910)
mean(glm.pred == Direction.0910)
```

###LDA with Lag2 interaction with Lag1
```{r}
lda.fit <-  lda(Direction ~ Lag2:Lag1, data = Weekly, subset = train)
lda.pred <-  predict(lda.fit, Weekly.0910)
mean(lda.pred$class == Direction.0910)
```

###QDA with sqrt(abs(Lag2))
```{r}
qda.fit <-  qda(Direction ~ Lag2 + sqrt(abs(Lag2)), data = Weekly, subset = train)
qda.class <-  predict(qda.fit, Weekly.0910)$class
table(qda.class, Direction.0910)
mean(qda.class == Direction.0910)
```

###KNN k =10
```{r}
knn.pred <-  knn(train.X, test.X, train.Direction, k = 10)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)
```

###KNN k = 100
```{r}
knn.pred <-  knn(train.X, test.X, train.Direction, k = 100)
table(knn.pred, Direction.0910)
mean(knn.pred == Direction.0910)
```

###Out of these permutations, the original LDA and logistic regression have better performance in terms of test error rate.

#Question 2.
##a. Remove the observations for whom the salary information is unknown, and then log-transform the salaries.

##b. Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.

##c. Perform boosting on the training set with 1000 trees for a range of values of the shrinkage parameter ??. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.

##d. Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.

##Ans a-d: We prepare the data and apply boosting on a range of shrinkage parameters, plotting both the train and test MSE values resulting from the proposed shrinkage. The resulting plot is displayed

```{r}
library(ISLR)
full.hit <- Hitters[!is.na(Hitters$Salary),]
full.hit$Salary <- log(full.hit$Salary)

tr <- sample(1:nrow(full.hit), 200)
hit.tr <- full.hit[tr,]
hit.te <- full.hit[-tr,]

shrinkage <- seq(0,0.03,0.00005)
tr.mse <- array(NA,length(shrinkage))
te.mse <- array(NA,length(shrinkage))

library(gbm)
for (i in 1:length(shrinkage)) {
  hit.boost <- gbm(Salary ~., data=hit.tr, distribution='gaussian',
        n.trees=1000, shrinkage=shrinkage[i], verbose=F)
    tr.mse[i] <- mean((predict(hit.boost, hit.tr, n.trees=1000) - hit.tr$Salary)^2)
    te.mse[i] <- mean((predict(hit.boost, hit.te, n.trees=1000) - hit.te$Salary)^2)
}
plot(tr.mse)
plot(te.mse)
```

##e. Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches.
##Ans: We compare the boosting's minimum test error of 0.19 with the test error with that of a simple linear model, and also lasso regression. Linear regression obtains a slightly better MSE than lasso, which is unsurprising as we have a much larger number of samples than covariates so we don't really need to enforce sparsity. Nevertheless the MSE of linear regression, 0.39, is vastly greater than boosting's 0.19. Boosting wins.

```{r}
min(te.mse)
```

###Linear Regression

```{r}
hit.lm <- lm(Salary ~., hit.tr)
mean((predict(hit.lm, hit.te) - hit.te$Salary)^2)

library(glmnet)
hit.las.cv <- cv.glmnet(as.matrix(hit.tr[,-c(19,20,14,15)]),
                        as.matrix(hit.tr[,19]), alpha=1)

hit.las <- glmnet(as.matrix(hit.tr[,-c(19,20,14,15)]),
                  as.matrix(hit.tr[,19]), alpha=1, lambda=hit.las.cv$lambda.min)
mean((predict(hit.las,
              as.matrix(hit.te[,-c(19,20,14,15)])) - hit.te$Salary)^2)
```

##f. Which variables appear to be the most important predictors in the boosted model?

##g. Now apply bagging to the training set. What is the test set MSE for this approach?

##Ans f,g: We investigate the variable importance by plotting our boosting model, using the summary() command.

```{r}
summary(hit.boost)
```

##We see the three most influential variables are CRuns, CAtBat, CRBI. These are all career statistics; so how many runs the player has made, career bats, runs batted in, in respective order, the most influential information pertaining to his salary.

##We apply bagging to the training set, using random forests.

```{r}
library(randomForest)
hit.rf <- randomForest(Salary ~., hit.tr, mtry=(ncol(hit.tr)-1), importance=T)
mean((predict(hit.rf, hit.te) - hit.te$Salary)^2)
```

#Question 3.

```{r}
library(e1071)
set.seed(1)
x <- rnorm(100)
y <- 4 * x^2 + 1 + rnorm(100)
class <- sample(100, 50)
y[class] <- y[class] + 3
y[-class] <- y[-class] - 3
plot(x[class], y[class], col = "red", xlab = "X", ylab = "Y", ylim = c(-6, 30))
points(x[-class], y[-class], col = "blue")
```

##We fit a support vector classifier on the training data

```{r}
z <- rep(-1, 100)
z[class] <- 1
data <- data.frame(x = x, y = y, z = as.factor(z))
train <- sample(100, 50)
data.train <- data[train, ]
data.test <- data[-train, ]
svm.linear <- svm(z ~ ., data = data.train, kernel = "linear", cost = 10)
plot(svm.linear, data.train)
table(predict = predict(svm.linear, data.train), truth = data.train$z)
```

##The support vector classifier makes 6 errors on the training data. Next, we fit a support vector machine with a polynomial kernel.

```{r}
svm.poly <- svm(z ~ ., data = data.train, kernel = "polynomial", cost = 10)
plot(svm.poly, data.train)
table(predict = predict(svm.poly, data.train), truth = data.train$z)
```

##The support vector machine with a polynomial kernel of degree 3 makes 9 errors on the training data. Finally, we fit a support vector machine with a radial kernel and a gamma of 1.

```{r}
svm.radial <- svm(z ~ ., data = data.train, kernel = "radial", gamma = 1, cost = 10)
plot(svm.radial, data.train)
table(predict = predict(svm.radial, data.train), truth = data.train$z)
```

##The support vector machine with a radial kernel makes 0 error on the training data.Now, we check how these models fare when applied to the test data.

```{r}
plot(svm.linear, data.test)
table(predict = predict(svm.linear, data.test), truth = data.test$z)
plot(svm.poly, data.test)
table(predict = predict(svm.poly, data.test), truth = data.test$z)
plot(svm.radial, data.test)
table(predict = predict(svm.radial, data.test), truth = data.test$z)
```

##We may see that the linear, polynomial and radial support vector machines classify respectively 9, 6 and 1 observations incorrectly. So, radial kernel is the best model in this setting.

